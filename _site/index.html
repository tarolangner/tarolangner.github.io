<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Tensor Labbet &middot; A blog of deep learnings
    
  </title>

  
  <link rel="canonical" href="https://tensorlabbet.com/">
  

  <link rel="stylesheet" href="https://tensorlabbet.com/public/css/poole.css">
  <link rel="stylesheet" href="https://tensorlabbet.com/public/css/syntax.css">
  <link rel="stylesheet" href="https://tensorlabbet.com/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://tensorlabbet.com/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="https://tensorlabbet.com/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://tensorlabbet.com/atom.xml">


  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p><b>Tensor Labbet</b> presents a captivating blog with articles, reviews and unsolicited opinion pieces on the state of artificial intelligence research in academia and the industry</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item active" href="https://tensorlabbet.com/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://tensorlabbet.com/about/">About</a>
        
      
    
      
    

    
   
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2025. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Tensor Labbet</a>
            <small>A blog of deep learnings</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://tensorlabbet.com/2025/06/21/review-ai-engineering/">
        Reviewing “AI Engineering” by Chip Huyen
      </a>
    </h1>

    <span class="post-date">21 Jun 2025, Taro Langner</span>

    <p class="message">
    In this post: A book review <br />
    <i>(5 min read)</i><br />
    
</p>

<p>In January this year, <a href="https://huyenchip.com/">Chip Huyen</a> published her newest book <a href="https://www.amazon.com/dp/1098166302?&amp;linkCode=sl1&amp;tag=chiphuyen-20&amp;linkId=0a4e5ad4b14080d44c42640550a9291e&amp;language=en_US&amp;ref_=as_li_ss_tl"><em>‘AI Engineering’</em></a>, which quickly <a href="https://www.youtube.com/watch?v=98o_L3jlixw&amp;pp=ygUscHJhZ21hdGljIGVuZ2luZWVyIGFpIGVuZ2luZWVyaW5nIGNoaXAgaHV5ZW4%3D">made waves</a> online.</p>

<p>Having read her previous book <a href="https://www.amazon.com/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969?&amp;_encoding=UTF8&amp;tag=chiphuyen-20&amp;linkCode=ur2&amp;linkId=0a1dbab0e76f5996e29e1a97d45f14a5&amp;camp=1789&amp;creative=9325"><em>‘Designing Machine Learning Systems’</em> (2022)</a>, which I warmly recommend, I wondered what could possible remain to be covered in 500+ additional pages. It really turned out to be something entirely different, and this blog post will attempt a short review for anyone still curious about the book.</p>

<h2 id="machine-learning-vs-ai-engineering">Machine Learning vs AI Engineering</h2>
<p>Her previous book, <em>‘Designing Machine Learning Systems’</em>, was a gentle but comprehensive overview of machine learning terms, techniques and applications that went easy on mathematical notation. While it briefly mentioned Large Language Models (LLMs) and remains highly relevant, it was published about half a year before the release of ChatGPT in November 2022, pre-dating its impact on the field.</p>

<p>The new book <em>‘AI Engineering’</em> is now entirely dedicated to working with LLMs. Subtitled <em>‘Building Applications with Foundation Models’</em>, it addresses a much wider audience than just ML Engineers or Researchers with a technical or scientific interest. Indeed, it clearly sets apart ML from the scope of the book and expects little prior knowledge of the field. The contents have therefore hardly any overlap with the previous book and are mostly complementary.</p>

<h2 id="format-and-style">Format and Style</h2>
<p>The field is now moving faster than anyone could hope to read about, with sensational new announcements almost every week. Technical specifics are quickly outdated and the book accordingly sticks to more timeless high-level concepts, insights and approaches. These are nonetheless often supported by hard evidence from statistics or relevant papers and first-hand accounts from industry experts. However, this also tends to make it a quite verbose, lighter read of often more ‘qualitative’ nature, with plenty of examples and a minimum of mathematical notation and formulas.</p>

<h2 id="content">Content</h2>
<p>The author has an hopeful but grounded take on the capabilities of LLMs, with the credentials to back it up. The book provides many use cases and guides, but also known failure modes backed by the scientific literature, along with techniques to mitigate them.</p>

<p>The contents include concepts such as pre- and post-training, retrieval-augmented generation (RAG), agents, tool-use, evaluation techniques, emergent properties and quirks of the models. It offers strategies for application development, an in-depth chapter on fine-tuning with techniques such as Low-Rank Adaptation (LoRA) as well as optimization strategies for inference and more.</p>

<p>The facts laid out in the book are well-researched and appear solid overall. Only one assertion struck me as wrong, which is the often repeated claim that the 2012 <a href="https://dl.acm.org/doi/abs/10.1145/3065386">AlexNet paper</a> was the first to utilize GPUs for training of neural networks. As mentioned in my previous <a href="https://tensorlabbet.com/#AlexNet">summary on AlexNet</a> the truth seems <a href="https://www.deeplearningbook.org/contents/applications.html">somewhat more nuanced</a>. Chances are that the author already has a rebuttal waiting in their inbox by 
<a href="https://arxiv.org/abs/2212.11279">Jürgen Schmidhuber</a>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The book provides a comprehensive overview and also offers many insights that were new to me, from the <a href="https://arxiv.org/html/2403.18932v1">mapped-out political leanings of LLMs</a> over the availability of <a href="https://commoncrawl.github.io/cc-crawl-statistics/plots/languages.html">training material in different languages</a> to prompt injection attacks that could leak sensitive training data when tasked to merely <a href="https://arxiv.org/abs/2311.17035">repeat the word ‘poem’ an infinite number of times</a>.</p>

<p>The distinction between ML and AI Engineering itself is thought-provoking. It illustrates how ML-based capabilities which used to be academic research topics are rapidly becoming applicable, abstracted and commoditized via APIs.</p>

<p>As with any other API, these capabilities thereby become accessible even without any deeper understanding of the field. The latter nonetheless helps, as simple LLM wrappers still encounter many limitations which are laid out in the book (hallucinations, costs, prompt injection attacks etc) and are still rarely competitive for hard engineering challenges (as seen e.g. in Kaggle challenges).</p>

<p>The book provides a solid foundation in this regard. Even with a more technical background, its balance between breadth and depth covered many gaps in my knowledge and makes it likely to remain relevant for years to come, so that it was well worth reading for me.</p>

<p><em>Disclaimer: I have no affiliation with the author and this review is not monetized, sponsored or funded in any other way. Originally, this post was meant to also review the book <a href="https://www.sscardapane.it/alice-book/">‘Alice’s Adventures in a Differentiable Wonderland’</a> by Simone Scardapane that I read earlier, but that will remain for a future post.</em></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://tensorlabbet.com/2025/01/29/radiology-image-segmentation/">
        What Deep Learning can do for Image Segmentation in Radiology
      </a>
    </h1>

    <span class="post-date">29 Jan 2025, Taro Langner</span>

    <p class="message">
    In this post: From Fully Convolutional Networks to TotalSegmentator <br />
    <i>(10 min read)</i><br />
    
</p>

<p>Amidst the ongoing hype around the growing capabilities of large language models, it can be curious to note how earlier predictions about machine learning have stood the test of time.</p>

<p>Autonomous driving and radiology in particular were considered obvious candidates for automation, starting with the deep learning boom for image recognition around 2012. Geoffrey Hinton famously suggested in 2016 that training of radiologists should be discontinued altogether, arguing that they <a href="https://www.youtube.com/watch?v=2HMPRXstSvQ">would be obsolete within five years</a>.</p>

<details>
<summary>And yet, things turned out quite different... (click to expand)</summary> 
<img src="/assets/radiologists_in_regular_cars.jpeg" alt="Radiologist Jobs in 2025" />

<i>
<a href="https://x.com/olexandr/status/1809670998746161426">(Source)</a>.
And once at work, a US radiologist in 2025  <a href="https://www.glassdoor.com/Salaries/physician-radiologist-salary-SRCH_KO0,21.html">may earn $265-495k per year</a> or pick from a number of job openings that actually <a href="https://x.com/erikbryn/status/1662564885589561344/photo/1">appears to be increasing</a>.
</i>
<br />

</details>
<p><br /></p>

<p>This is a sobering reminder that many real-world problems turned out to be much harder to crack than expected at first. Although the AI hype has since mostly turned elsewhere, a closer look at what happened in these fields can be rather interesting.</p>

<p>This blog post examines one of their most active, and arguably most successful research areas and reviews one decade worth of progress around deep learning methods for semantic segmentation of medical images in radiology.</p>

<h2 id="semantic-segmentation-for-radiology">Semantic Segmentation for Radiology</h2>

<p>From magnetic resonance imaging (MRI) to computed tomography (CT), medical imaging offers various modalities for visualising the human body. Just like the anatomy itself, these images are often three-dimensional and thus composed of volumetric pixels, or voxels, similar to the block worlds of Minecraft.</p>

<p>Early on, measurements and findings were often reported from two-dimensional X-ray images. With increasingly affordable imaging technology, the data has since grown both in quantity and resolution, leaving <a href="https://www.sciencedirect.com/science/article/pii/S1076633215002457?casa_token=Pud-r9Fq_0wAAAAA:2Vb-4PygjIL9U4FDzfmQ_I9g1cqrvSo-UP1KK9tanNCZalCEN_OUtMCwKdFXgCliL0bXCb9TErwI">mere seconds on average</a> for a radiologist to inspect images that can each consist of millions of voxels.</p>

<p><em>Semantic segmentation</em> is one type of image analysis that is commonly performed in research and industry on these images. It aims to assign a class label to every pixel or voxel, typically to mark all parts of the image that contain a certain tissue or structure. Once complete, a segmentation mask can be used to measure volumes, render surface models or plan radiation treatments.</p>

<p class="message">

<b>Video Example:</b> <br />

<a href="https://www.youtube.com/watch?v=ZRYMItzwg8g&amp;t=220s">- Manual CT Image Segmentation in 3D Slicer (YouTube)</a> [7:43 minutes] <br />
</p>

<p>When done by hand, a given volumetric image is typically segmented by drawing on it as a stack of dozens or hundreds of two-dimensional slices. This can take from minutes to hours or <a href="https://www.cell.com/neuron/fulltext/S0896-6273(02)00569-X?cc=y%3Fcc%3Dy">even days</a>. Results may vary not only between different operators but also when the same image is analysed repeatedly by the same person. This becomes a challenge in studies where hundreds or even thousands of scans are to be analysed in this way.</p>

<p>Despite its many issues, manual segmentation remains the method of choice for many real-world projects that operate in risk-averse settings under restrictive regulatory constraints. On the flip side, these same regulations and concerns have helped to reduce the number of scenarios where malfunctioning software would <a href="https://en.wikipedia.org/wiki/Therac-25">burn patients with radiation</a> or <a href="https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfRes/res.cfm?id=171315">confuse surgeons with misleading 3d navigation views</a>.</p>

<p>Risk-averse skeptics were furthermore proven right on several occasions before when they doubted claims about technology being superior to medical experts. In the late 90s, computer-aided detection (CAD) systems were funded with millions of dollars per year but later reported to provide no benefit or <a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2443369">perhaps even cause harm</a>. The deep learning boom later caused an entire flood of such claims, like in the 2017 <a href="https://arxiv.org/abs/1711.05225">CheXNet paper</a> co-authored by Andrew Ng, the issues of which are discussed with many insights in the <a href="https://laurenoakdenrayner.com/2018/01/24/chexnet-an-in-depth-review/">blog of Lauren Oakden-Rayner</a>.</p>

<p>Despite this history of overpromising, the work done by a multitude of researchers and engineers over the years has nonetheless achieved some impressive progress. Especially deep learning systems for semantic segmentation of medical images have a lot to offer and are worth a closer look.</p>

<h2 id="deep-learning-for-semantic-image-segmentation">Deep Learning for Semantic Image Segmentation</h2>

<p>In the ImageNet challenge of 2012, <a href="https://tensorlabbet.com/#AexNet">AlexNet</a> famously set a new benchmark result for image recognition by assigning one of 1,000 possible class labels to a given input image with unprecedented accuracy.</p>

<p><em>Sliding window segmentation</em> used such image classifiers for semantic segmentation, applying them to each position of an image to receive as input a patch around the current position and predict a class label for the central pixel. This approach suffered from inefficiencies, however, with redundant processing wherever a given image area appeared in multiple, adjacent patches.</p>

<p><a href="https://openaccess.thecvf.com/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html">[Fully Convolutional Networks, 2015]</a> were proposed as a neural network architecture for dense prediction of pixel-wise labels. This approach removes any fully-connected layers from established architectures for image classification. The remaining convolutional and pooling layers act as an encoder, producing feature maps that retain spatial image information at different resolutions. From these, a 1x1 convolution produces one feature map for each class, to be upsampled by transposed convolution layers to restore the original image dimensions.</p>

<p class="message">

<b>Transposed Convolution (Animations)</b> <br />

<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">- Convolution arithmetic (GitHub)</a> by Vincent Dumoulin,  Francesco Visin <br />
<br />

Note: Transposed convolutions differ from dilated (or à trous) convolutions <a href="https://tensorlabbet.com/#DilatedConv">that featured in a previous blog post</a>.

</p>

<p>With skip connections, these upsampled feature maps are obtained not only from the final, most low-resolution output, but also from earlier steps and fused together by summation to incorporate more high-resolution features.</p>

<p><a href="http://miai.ha.edu.cn/Essays/8Medical%20image%20segmentation/U-Net.pdf">[U-Net, 2015]</a> built on this approach by proposing a symmetric encoder-decoder architecture with even more skip connections. The encoder part forms the left half of its U-shape, with successive network layers producing feature maps of decreasing resolution. The decoder path then gradually restores the original resolution with transposed convolution layers. Long skip connections provide shortcuts that concatenate feature maps of the encoder to their counterparts in the decoder.
This enables U-Nets to consider both coarse, low-resolution features as well as detailed, high-resolution features.</p>

<p><a href="https://lmb.informatik.uni-freiburg.de/Publications/2016/CABR16/cicek16miccai.pdf">[3D U-Net, 2016]</a> later extended these concepts to volumetric, voxel-based input data by using 3D variants of both pooling and convolution layers.</p>

<p>U-Net architectures enjoyed enormous success and remain competitive options for medical image segmentation to this day. They tend to be robust and reliable, with 2D variants training within minutes on modern GPUs and being lightweight enough for inference even on laptop CPUs.
They can also perform well even with just a few dozen training images, as each pixel or voxel effectively forms one training sample. Hundreds of U-Net variants were subsequently proposed in the literature, including <a href="https://arxiv.org/abs/1810.11654">SegResNet</a> with short skip connections and <a href="https://arxiv.org/abs/1801.05746">2.5D approaches</a> that stack adjacent slices to form RGB colour images suitable for encoders pre-trained on ImageNet.</p>

<p><a href="https://arxiv.org/abs/1809.10486">[nnU-Net, 2018]</a> (‘no-new-Net’) was ultimately proposed as a self-adapting framework for training effective U-Net architectures. It enables the training of both 2D and 3D U-Nets, as well as cascades with subsequent segmentation steps. Instead of focusing on modifications of the model architecture, it adjusts the preprocessing, training, inference and post-processing with many domain-specific heuristics and also enables cross-validations for evaluation.</p>

<p>For example, as imaging devices of different vendors can vary in contrast, the preprocessing first standardizes or at least scales the image intensities. Some segmentation tasks suffer from extreme class imbalances (for example when small cancerous lesions are to be segmented) and the sampling strategy therefore tries to balance the foreground and background samples for each minibatch in training. Variations in patient anatomy and position are simulated by augmentation with rotation, scaling and mirroring during training. Test-time augmentation furthermore presents each sample with its mirrored copy and averages the predictions for increased robustness. Contemporary GPUs were often limited to 11GB, and so the inference processes larger images by blending patch-wise predictions.</p>

<p>Its authors at the German Cancer Research Center (DKFZ) published an open-source implementation. Its <a href="https://github.com/MIC-DKFZ/nnUNet">research code</a> (which purportedly <a href="https://github.com/MIC-DKFZ/batchgenerators/blob/1185d57bbc002f4b88c03fdea885dc28537ad8e7/batchgenerators/augmentations/noise_augmentations.py#L55">left Godzilla dead</a>, whereas I was merely scarred) enabled many to reproduce these techniques. So successful was this framework both in benchmark challenges and various research papers that even in summer 2024 it <a href="https://arxiv.org/pdf/2404.09556">still lays claim</a> to dominance in this domain.</p>

<p><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10546353/">[TotalSegmentator, 2023]</a> was later released as a freely available, already trained nnU-Net model for segmentation of 104 different structures in CT images, such as organs, bones, muscle and blood vessels. Up to this point, it was common that segmentation models trained on one dataset would not perform as well on data from other sources due to distribution shifts from different imaging devices, protocols or patient demographics. By training on a varied dataset of over one thousand real-world CT images with different age groups, sites, and protocols, TotalSegmentator made a substantial leap in generalization across arbitrary CT data. Notably, the model was made highly accessible with a Python package, integration into 3D Slicer and even a <a href="https://totalsegmentator.com/">free web interface</a>. In 2024, <a href="https://arxiv.org/pdf/2405.194929">an extended version</a> was released for segmentation of 59 structures in even more variable images from MRI.</p>

<h2 id="concluding-thoughts">Concluding Thoughts</h2>

<p>From early Fully Convolutional Networks of 2014 to TotalSegmentator in 2024 for MRI, these papers trace the evolution of deep learning for semantic segmentation of radiology images over an entire decade. As an open research question it motivated thousands of papers with varied methodologies. Their insights were gradually distilled into the later publications and methods. Today, no programming or deep learning knowledge is required for anyone to simply drag and drop an image into <a href="https://totalsegmentator.com/">TotalSegmentator</a> with impressive results. <br />
That is progress!</p>

<p>Medical image segmentation still remains an active field of research, with various benchmark challenges (beyond the scope of TotalSegmentator) in conferences like <a href="https://miccai.org/index.php/">MICCAI</a> and <a href="https://www.kaggle.com/competitions/rsna-2024-lumbar-spine-degenerative-classification">Kaggle challenges with monetary prizes</a>.</p>

<p>The convolutional neural network architectures reviewed so far in this post also remain a competitive option especially for limited training data, as is common for medical images. Methods that utilise Transformers have emerged too, such as <a href="https://arxiv.org/abs/2201.01266">SwinUNETR</a> (also see the open-source <a href="https://github.com/Project-MONAI/tutorials/blob/main/auto3dseg/README.md">MONAI</a> framework) and <a href="https://www.nature.com/articles/s41467-024-44824-z">MedSAM</a> as a medical version of the <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html">Segment Anything Model</a>. More recently, even multimodal <a href="https://pubs.rsna.org/doi/pdf/10.1148/radiol.232756">large language models are being considered for radiology tasks</a> which may warrant an entire blog post of their own.</p>

<p>So with all these innovations, then, why has radiology not been automated yet? While there is a growing number of success stories of FDA-approved and CE marked supporting tools entering the market, these systems have to overcome numerous obstacles. Next to regulatory hurdles, workflow integration and acceptance issues, the technical challenge is just one of them.</p>

<p>However, even the technical challenges in this space have not been truly automated yet. I received a taste of this myself when working on <a href="https://www.nature.com/articles/s41598-020-77981-4">kidney segmentations in MRI of UK Biobank</a>. Before my results for 40,000 participants could be uploaded to the <a href="https://biobank.ndph.ox.ac.uk/showcase/label.cgi?id=159">official data catalogue</a>, I skimmed through thousands of these images to identify and understand outlier cases where my U-Net had failed to accurately segment both kidneys.</p>

<p>This taught me about <a href="https://en.wikipedia.org/wiki/Horseshoe_kidney">horseshoe kidneys</a>, or renal fusion, in which both kidneys are connected from birth. In this large dataset over a dozen such cases occurred, and without any training examples for this rare condition, the model had not learned how to handle them well at first. Mel Gibson is often given as a famous case of renal fusion, and if he had participated in UK Biobank, my system would have likely failed him. But who knows, perhaps today TotalSegmentator would have succeeded even for him?</p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://tensorlabbet.com/2024/11/11/lost-reading-items/">
        The Lost Reading Items
      </a>
    </h1>

    <span class="post-date">11 Nov 2024, Taro Langner</span>

    <p class="message">
    In this post: An attempt to reconstruct Ilya Sutskever's 2020 AI reading list <br />
    <i>(8 min read)</i><br />
    
</p>

<p>I recently <a href="https://tensorlabbet.com/2024/09/24/ai-reading-list/">shared a summary</a> of a <a href="https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE">viral AI reading list</a> attributed to Ilya Sutskever, which laid claim to covering ‘<em>90% of what matters</em>’ back in 2020. It boils down the reading items to barely one percent of the original word count to form the TL;DR I would have wished for before reading.</p>

<p>The viral version of the list as shared online is known to be incomplete, however, and includes <a href="https://x.com/andrew_n_carr/status/1752526711311507526">only 27</a> of <a href="https://dallasinnovates.com/exclusive-qa-john-carmacks-different-path-to-artificial-general-intelligence/?utm_source=www.turingpost.com&amp;utm_medium=referral&amp;utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">about 40</a> original reading items.
The rest <a href="https://news.ycombinator.com/item?id=34641359">allegedly</a> fell victim to the E-Mail deletion policy at Meta¹. These missing reading items have inspired some good discussions in the past, with many different ideas as to which papers would have been important enough to include.</p>

<p>This post is an attempt to identify these lost reading items. It builds on clues gathered from the viral list, contemporary presentations given by Ilya Sutskever, resources shared by OpenAI and more.</p>

<p>¹<em>Correction: An earlier version mistakenly referred to OpenAI here instead of Meta</em></p>

<h2 id="filling-the-gaps">Filling the Gaps</h2>

<p>The main piece of evidence is <a href="https://x.com/andrew_n_carr/status/1752526711311507526">a claim shared along with the list</a> according to which an entire selection of <em>meta-learning</em> papers was lost.</p>

<p>Meta-learning is often said to pursue <em>‘learning to learn’</em>, with neural networks being trained for a general ability to adapt more easily to new tasks for which only few training samples are available. A network should thus be able benefit from its existing weights without requiring an entirely new training from scratch on the new data. <em>One-shot learning</em> provides just a single training sample to a model from which it is expected to learn a new downstream task, whereas <em>zero-shot</em> settings provide no annotated training samples at all.</p>

<p>For some of the candidate papers listed below, the case can be strengthened further by evidence in the form of an endorsement straight from OpenAI itself. Ilya Sutskever was chief scientist at a time when OpenAI published the educational resource <a href="https://spinningup.openai.com/en/latest/index.html">‘Spinning Up in Deep RL’</a> which includes several of these candidates in an entirely separate reading list of 105 <a href="https://spinningup.openai.com/en/latest/spinningup/keypapers.html#meta-rl">‘Key Papers in Deep RL’</a>. Below, the papers which also appear in that list are marked with a symbol (⚛).</p>

<h3 id="clues-from-the-preserved-reading-items">Clues from the Preserved Reading Items</h3>

<p>Some meta-learning concepts can be found even in the known parts of the list.
The preserved reading items can be arranged into a narrative arc around a related branch of research on <a href="https://arxiv.org/abs/1410.3916">Memory-Augmented Neural Networks (MANNs)</a>. Following the <a href="https://tensorlabbet.com/2024/09/24/ai-reading-list/#NTM">‘Neural Turing Machine’ (NTM)</a> paper, <a href="https://tensorlabbet.com/2024/09/24/ai-reading-list/#Set2Set">‘Set2Set’</a> and <a href="https://tensorlabbet.com/2024/09/24/ai-reading-list/#RelationalRNN">‘Relational RNNs’</a> experimented with external memory banks that an RNN could read and write information on. They directly cite or closely relate to several papers which may well have been part of the original list:</p>

<p><strong>Potential Reading Items (Part 1):</strong></p>

<ul>
  <li><a href="http://proceedings.mlr.press/v48/santoro16.pdf">‘Meta-learning with memory-augmented neural networks’</a> <br />
<em>from 2016</em> 
<!-- <span style="color:gray">Adapts NTM for one-shot classification of [Omniglot](https://www.cs.cmu.edu/~rsalakhu/papers/LakeEtAl2015Science.pdf) characters</span> --></li>
  <li><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/cb8da6767461f2812ae4290eac7cbc42-Paper.pdf">‘Prototypical networks for few-shot learning’</a> <br /> 
<em>from 2017</em> 
<!-- Learn embedding spaces for few- and zero-shot classification  --></li>
  <li><a href="http://proceedings.mlr.press/v70/finn17a/finn17a.pdf">‘Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks’</a>⚛ <br />
<em>from 2017</em></li>
</ul>

<h3 id="clues-from-contemporary-presentations">Clues from Contemporary Presentations</h3>

<p>Certain papers about meta-learning and <em>competitive self-play</em> also feature repeatedly in a series of presentations held by Ilya Sutskever around this time and may well have eventually been included in the reading list too.</p>

<p class="message">

<b>Recorded Presentations:</b> <br />

<a href="https://www.youtube.com/watch?v=BCzFs9Xb9_o">- Meta Learning and Self Play - Ilya Sutskever, OpenAI (YouTube)</a>, 2017 <br />
<a href="https://www.youtube.com/watch?v=AopSlxNYqX8">- OpenAI - Meta Learning &amp; Self Play - Ilya Sutskever (YouTube)</a>, 2018 <br />
<a href="https://www.youtube.com/watch?v=9EN_HoEk3KY">- Ilya Sutskever: OpenAI Meta-Learning and Self-Play (YouTube)</a>, 2018 <br />
</p>

<p>These presentations largely overlap and repeatedly reference known contents of the reading list. They open with a fundamental motivation of why deep learning works, framing backpropagation with neural networks as a search for <em>small circuits</em> that relate to the <a href="https://tensorlabbet.com/2024/09/24/ai-reading-list/#MDL">Minimum Description Length principle</a>, according to which the shortest program that can explain given data will reach the best generalization possible.</p>

<p>Next, all three presentations reference the following meta-learning papers:</p>

<p><strong>Potential Reading Items (Part 2):</strong></p>
<ul>
  <li><a href="https://amygdala.psychdept.arizona.edu/labspace/JclubLabMeetings/Lijuan-Science-2015-Lake-1332-8.pdf">‘Human-level concept learning through probabilistic program induction’</a> <br /> 
as <em>Lake et al., 2016</em></li>
  <li><a href="https://arxiv.org/pdf/1611.01578">‘Neural Architecture Search with Reinforcement Learning’</a> <br /> 
as <em>Zoph and Le, 2017</em></li>
  <li><a href="https://arxiv.org/pdf/1707.03141">‘A Simple Neural Attentive Meta-Learner’</a>⚛ <br /> 
as <em>Mishra et al., 2017</em></li>
</ul>

<p>Reinforcement Learning (RL) also features heavily in all three presentations, with close links to meta-learning. One key concept is <em>competitive self-play</em> in which agents interact in a simulated environment to reach specific, typically adversarial objectives. As a way to <em>‘turn compute into data’</em>, this approach enabled simulated agents to outperform human champions and invent new moves in rule-based games. Ilya Sutskever presents an evolutionary biology perspective that relates competitive self-play to the impact of social interaction on brain size <a href="https://www.science.org/doi/10.1126/science.1098410">(pay-walled link)</a>.
He goes on to suggest that rapid competence gain in a simulated <em>‘agent society’</em> may ultimately, according to his judgement, provide <a href="https://www.youtube.com/watch?v=9EN_HoEk3KY&amp;t=2325s">a plausible path towards a form of AGI</a>.</p>

<p>Given the significance he ascribes to these concepts, it seems plausible that some of the cited papers on self-play may have later also been included in the reading list. They may form a sizeable chunk of the missing items, especially as RL is otherwise mentioned by <a href="https://tensorlabbet.com/2024/09/24/ai-reading-list/#MachineSuperIntelligence">only one</a> of the preserved reading items.</p>

<p><strong>Potential Reading Items (Part 3):</strong></p>

<ul>
  <li><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf">‘Hindsight Experience Replay’</a>⚛ <br /> 
as <em>Andrychowicz et al., 2017</em></li>
  <li><a href="https://arxiv.org/abs/1509.02971">‘Continuous control with deep reinforcement learning’</a>⚛ <br />
as <em>DDPG: Deep Deterministic Policy Gradients, 2015</em></li>
  <li><a href="https://arxiv.org/abs/1710.06537">‘Sim-to-Real Transfer of Robotic Control with Dynamics Randomization’</a> <br />
as <em>Peng et al., 2017</em></li>
  <li><a href="https://arxiv.org/abs/1710.09767">‘Meta Learning Shared Hierarchies’</a> <br />
as <em>Frans et al., 2017</em></li>
  <li><a href="https://www.csd.uwo.ca/~xling/cs346a/extra/tdgammon.pdf">‘Temporal Difference Learning and TD-Gammon [1995]’</a> <br />
as <em>Tesauro et al., 1992</em></li>
  <li><a href="https://www.youtube.com/watch?v=JBgG_VSP7f8&amp;t=2s">‘Karl Sims - Evolved Virtual Creatures, Evolution Simulation, 1994’</a> <br />
as <em>Carl Sims, 1994 (YouTube video [4:09])</em></li>
  <li><a href="https://arxiv.org/abs/1710.03748">‘Emergent Complexity via Multi-Agent Competition’</a> <br /> 
as <em>Bansal et al., 2017</em></li>
  <li><a href="https://arxiv.org/abs/1706.03741">‘Deep reinforcement learning from human preferences’</a>⚛ <br /> 
as <em>Christiano et al., 2017</em> (Note: Introduces RLHF)</li>
</ul>

<p>Even today, these presentations from around 2018 are still worth watching. Next to fascinating bits of knowledge, they also include gems such as the statement:</p>
<blockquote>
  <p><a href="https://www.youtube.com/watch?v=BCzFs9Xb9_o&amp;t=2532s">‘Just like in the human world: The reason humans find life difficult is because of other humans’</a></p>
  <div style="text-align: right">-Ilya Sutskever </div>
</blockquote>

<p>While some concepts in computer science accordingly appear timeless, other points may seem surprising today, like the casual remark of an audience member in the Q&amp;A session:</p>
<blockquote>
  <p><a href="https://www.youtube.com/watch?v=9EN_HoEk3KY&amp;t=3082s">‘It seems like an important sub-problem on the path to AGI will be understanding language, and the state of generative language modelling right now is pretty abysmal.’</a></p>
  <div style="text-align: right">-Audience member </div>
</blockquote>

<p>To which Ilya Sutskever responds:</p>
<blockquote>
  <p><a href="https://www.youtube.com/watch?v=9EN_HoEk3KY&amp;t=3106s">‘Even without any particular innovations beyond models that exist today, simply scaling up models that exist today on larger datasets is going to go surprisingly far.’</a></p>
  <div style="text-align: right">-Ilya Sutskever (in 2018)</div>
</blockquote>

<p>This response was later confirmed by experimental results in the reading item <a href="https://tensorlabbet.com/2024/09/24/ai-reading-list/#ScalingLaws">‘Scaling Laws for Neural Language Models’</a> (which echoes the <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">‘Bitter Lesson’</a> by Rich Sutton). It was ultimately proven true, as he would oversee Transformer architectures scaled up to <a href="https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/">an estimated 1.8 trillion parameters and costing over $60 million to train on 128 GPUs</a> forming Large Language Models (LLMs) which are today capable of generating text that is increasingly difficult to distinguish from human writing.</p>

<h2 id="honorable-mentions">Honorable Mentions</h2>

<p>Many other works and authors may have featured on the original list, but the evidence wears increasingly thin from here on.</p>

<p>Overall, the preserved reading items manage to strike an impressive balance between covering different model classes, applications and theory while also including many famous authors of the field. Perhaps the exceptions to this rule are worth noting, even if they may have slipped among the <em>‘10% of what matters’</em> that didn’t make the original list.</p>

<p>As such, it would have seemed plausible to include:</p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Yann_LeCun">Yann LeCun</a> with <a href="https://hal.science/hal-03926082/document">pioneering work on CNNs</a> for real-world use</li>
  <li><a href="https://en.wikipedia.org/wiki/Ian_Goodfellow">Ian Goodfellow</a> with <a href="https://proceedings.neurips.cc/paper_files/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html">Generative Adversarial Networks (GANs)</a> that dominated image generation at the time and</li>
  <li><a href="https://en.wikipedia.org/wiki/Demis_Hassabis">Demis Hassabis</a> for <a href="https://daiwk.github.io/assets/dqn.pdf">RL research</a> towards <a href="https://ccsp.hms.harvard.edu/wp-content/uploads/2020/11/AlphaFold-at-CASP13-AlQuraishi.pdf">AlphaFold</a> that earned a Nobel prize</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>This post will remain largely speculative until more becomes known. After all, even the viral list itself was never officially confirmed to be authentic. Nonetheless, the potential candidates for the lost reading items listed above seemed worth sharing. Taken together, they may well fill a gap in the viral version of the list that would, in the words of the author, corresponded roughly to a missing <em>‘30% of what matters’</em> at its time.</p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://tensorlabbet.com/2024/09/24/ai-reading-list/">
        Summary of Ilya Sutskever's AI Reading List
      </a>
    </h1>

    <span class="post-date">24 Sep 2024, Taro Langner</span>

    <p class="message">
    In this post: Ilya Sutskever's AI Reading list in ~120 words per item <br />
    
    <i>(15 min read)</i>
</p>

<p>Earlier this year, a reading list with about 30 papers <a href="https://x.com/andrew_n_carr/status/1752526711311507526">was shared on Twitter</a>.<br />
 It reportedly forms part of a longer version originally compiled by Ilya Sutskever, co-founder and chief scientist of OpenAI at the time, for John Carmack in 2020 with the remark:</p>
<blockquote>
  <p><a href="https://dallasinnovates.com/exclusive-qa-john-carmacks-different-path-to-artificial-general-intelligence/?utm_source=www.turingpost.com&amp;utm_medium=referral&amp;utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations"><em>‘If you really learn all of these, you’ll know 90% of what matters’</em>.</a></p>
</blockquote>

<p>While <a href="https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE">the list</a> is fragmentary and much has happened in the field since, this endorsement and the claim that it was part of onboarding at OpenAI quickly made it go <a href="https://news.ycombinator.com/item?id=40397806">somewhat</a> <a href="https://old.reddit.com/r/ArtificialInteligence/comments/1cpbh1s/ilya_sutskever_if_you_really_learn_all_of_these/">viral</a>.</p>

<p>At about 300,000 words total, the combined content nonetheless corresponds to around one thousand book pages of dense, technical text and requires a decent investment in time and energy for self-study. 
After doing just that, I therefore dedicate this blog post to all those of us who provisionally bookmarked it (“for later”) and are still curious. What follows is my own condensed and structured summary with about 120 words per item, free of mathematical notation, to capture the essential key points, context and some perspective gained from reading it with the surrounding literature.</p>

<h2 id="in-a-nutshell">In a Nutshell</h2>

<p>The list contains 27 reading items, with papers, blog posts, courses, one dissertation and two book chapters, all originally dating from 1993 to 2020.</p>

<p>The contents can be roughly broken down as follows:</p>

<table>
  <thead>
    <tr>
      <th>Methodology</th>
      <th style="text-align: right">Items</th>
      <th style="text-align: right">Share<a href="##" title="Percentage of total word count">*</a></th>
      <th>Topics</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Convolutional Neural Networks (CNNs)</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">25%</td>
      <td>image recognition, semantic segmentation</td>
    </tr>
    <tr>
      <td>Recurrent Neural Networks (RNNs)</td>
      <td style="text-align: right">10</td>
      <td style="text-align: right">19%</td>
      <td>language modeling, speech-to-text, machine translation, combinatorial optimization, visual question answering, content-based attention</td>
    </tr>
    <tr>
      <td>Transformers</td>
      <td style="text-align: right">3</td>
      <td style="text-align: right">6%</td>
      <td>multi-head and dot-product attention, language model scaling</td>
    </tr>
    <tr>
      <td>Information Theory</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">42%</td>
      <td>Kolmogorov complexity, compression, Minimum Description Length</td>
    </tr>
    <tr>
      <td>Miscellaneous</td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">8%</td>
      <td>variational inference, representation learning, graph neural networks, distributed training</td>
    </tr>
  </tbody>
</table>

<p>Using these categories, the next sections summarize the gist of each item, roughly sorted by how they build on each other.</p>

<h2 id="convolutional-neural-networks">Convolutional Neural Networks</h2>

<blockquote>
  <details>
<summary>CS231, 2017 <a name="CS231"></a> </summary> 
Stanford University Course <br /> 
Length: ~50,000 words, forming 11 blocks of 2 modules <br /> 
Instructors: Fei-Fei Li, Andrej Karpathy and Justin Johnson <br />
<a href="#CS231">🔗</a>
</details>
</blockquote>

<p><a href="https://cs231n.github.io/">[CS231, 2017]</a> is a classic course on deep learning fundamentals from Stanford University. It builds up from linear classifiers and their ability to learn a given task based on mathematical optimization, or training, which adjusts their internal parameter weights such that applying them to input data will produce more desirable outputs. This basic concept is developed into backpropagation for training of neural networks, in which trainable parameters are typically arranged into multiple layers together with other modules such as activation functions and pooling layers. <em>Convolutional Neural Networks</em> (CNNs) are introduced as a specialized architecture for image recognition, as used in modern computer vision systems to this day. Extended video lectures are <a href="https://www.youtube.com/watch?v=vT1JzLTH4G4&amp;list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk">available on youtube</a>.</p>

<p><em>Note: If you are starting from zero, this course and newer resources by e.g. <a href="(https://www.coursera.org/specializations/deep-learning)">DeepLearning.AI on Coursera</a> or <a href="https://course.fast.ai/">FastAI</a> will help you to get more out of the remaining list.</em></p>

<hr />

<blockquote>
  <details>
<summary>AlexNet, 2012 <a name="AlexNet"></a></summary> 
Paper <br /> 
Length: ~6,000 words<br /> 
Authors: Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton <br />
<a href="#AexNet">🔗</a>
</details>
</blockquote>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3065386">[AlexNet, 2012]</a> established CNNs as state of the art for image recognition and arguably initiated the widespread hype around deep learning. It outperformed its competitors in the 2012 ImageNet benchmark challenge, predicting whether a given input image contained e.g. a cat, dog, ship or any other of 1,000 possible classes, so conclusively that the real-world dominance of deep learning became commonly accepted. An important factor was its early* CUDA implementation that enabled unusually fast training on GPUs.</p>

<p><em>*Note: Earlier GPU implementations are documented in <a href="https://www.deeplearningbook.org/contents/applications.html">section 12.1.2</a> of the book <a href="https://www.deeplearningbook.org/">Deep Learning</a>.</em></p>

<hr />

<blockquote>
  <details>
<summary>ResNet, 2015 <a name="ResNet"></a></summary> 
Paper <br /> 
Length: ~6,000 words<br /> 
Authors: Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun <br />
<a href="#ResNet">🔗</a>
</details>
</blockquote>

<p><a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">[ResNet, 2015]</a> succeeded AlexNet as a more modern CNN architecture, reaching first place on the ImageNet challenge in 2015. It remains a popular CNN architecture to this day and is <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html">subject of ongoing research</a>. It introduced residual connections into CNN architectures that had become ever deeper, stacking more convolutional layers to achieve higher representational power. By allowing residual connections to skip or bypass entire blocks of layers, ResNet architectures suffered less from gradient degradation effects in training and could thus be robustly trained at previously unseen depth.</p>

<hr />

<blockquote>
  <details>
<summary>ResNet identity mappings, 2016 <a name="ResNetIdentity"></a></summary> 
Paper <br /> 
Length: ~6,000 words<br /> 
Authors: Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun <br />
<a href="#ResNetIdentity">🔗</a>
</details>
</blockquote>

<p><a href="https://arxiv.org/abs/1603.05027">[ResNet identity mappings, 2016]</a> were later proposed by the ResNet authors as a ‘clean’ information path and best design for the skip connections, so that their contents are merely added to the results of a bypassed block without any further modification. Whereas earlier designs placed an activation layer on the skip path after the addition, the proposed pre-activation design moves this layer to the start of the bypassed block instead. The skip connections can thus form a shortcut through the entire neural network that is only interrupted by additions, allowing improved propagation of gradient signals that make it possible for even deeper neural networks to be trained.</p>

<hr />

<blockquote>
  <details>
<summary>Dilated convolutions, 2015 <a name="DilatedConv"></a></summary> 
Paper <br /> 
Length: ~6,000 words<br /> 
Authors: Fisher Yu and Vladlen Koltun <br />
<a href="#DilatedConv">🔗</a>
</details>
</blockquote>

<p><a href="https://arxiv.org/abs/1511.07122">[Dilated convolutions, 2015]</a> (or <em>à trous</em> convolutions) were proposed as a new type of module for dense prediction with CNNs in tasks like semantic image segmentation, where class labels are assigned to any given pixel of an input image. Architectures such as AlexNet and ResNet condense input images to lower-dimensional representations via strided convolutions or pooling layers to predict one class label for an entire image. Related architectures for dense prediction therefore typically restore the original input image resolution from these downsampled, intermediate representations via upsampling operations. Whereas e.g. <a href="](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28)">transpose convolutions</a> achieve this with competitive results, dilated convolutions avoid downsampling entirely. Instead, they space out the filter kernel of a convolutional layer to skip one or more neighboring input pixels, thereby providing a larger receptive field without any reduction in resolution.</p>

<hr />

<h2 id="recurrent-neural-networks">Recurrent Neural Networks</h2>

<p>Today, <em>Recurrent Neural Networks</em> (RNNs) have been largely superseded by Transformers and date from what Ilya Sutskever himself would later call the <a href="Link: https://www.youtube.com/watch?v=Ft0gTO2K85A&amp;t=625s">“[pre-2017] stone age”</a> of machine learning. They nonetheless remain <a href="https://arxiv.org/abs/2405.04517">subject of active research</a> and see continued use in certain applications. Forming a substantial part of the reading list, they showcase the evolution of early insights and architectural developments that lead up to the systems of today. Most of the RNNs listed below are <a href="https://link.springer.com/chapter/10.1007/978-3-642-24797-2_4"><em>Long Short-Term Memory</em> (LSTM)</a> architectures. Some designs furthermore include <em>Feedforward Networks</em> with no recurrent connections, usually trained end-to-end as part of the model.</p>

<hr />

<blockquote>
  <details>
<summary>Understanding LSTM Networks, 2015 <a name="UnderstandingLSTMs"></a></summary> 
Blog Post <br /> 
Length: ~2,000 words<br /> 
Author: Christopher Olah <br />
<a href="#UnderstandingLSTMs">🔗</a>
</details>
</blockquote>

<p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">[Understanding LSTM Networks, 2015]</a> provides a brief introduction to RNNs and LSTMs in particular. RNNs can process a sequence of inputs, one step at a time, while evolving a hidden state vector that is (re-)ingested, updated and returned again at each step along the input sequence. The hidden state vector thereby allows for information to persist and be passed to subsequent processing steps. Nonetheless, simpler RNNs typically struggle with long-term dependencies. LSTMs alleviate this by introducing a cell state as additional recurrent in- and output, acting as a memory pathway for addition, update or removal of information along each processing step via trainable gating mechanisms.</p>

<hr />

<blockquote>
  <details>
<summary>The Unreasonable Effectiveness of RNNs, 2015 <a name="EffectiveRNNs"></a></summary> 
Blog Post <br /> 
Length: ~6,000 words<br /> 
Author: Andrej Karpathy <br />
<a href="#EffectiveRNNs">🔗</a>
</details>
</blockquote>

<p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">[The Unreasonable Effectiveness of RNNs, 2015]</a> shows use cases and results of RNNs in action. They are distinguished by their ability to both process and predict variable-sized sequences while also maintaining an internal state, prompting the author Andrej Karpathy to state <em>“If training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs.”</em>. He showcases results for image captioning and character-level language modeling that enables RNNs to automatically generate prose and articles. Early code generation capabilities are noted, with convincing syntax but failure to compile and a tendency to suffer from <em>hallucinations</em> where the model provides outputs as most probable that are evidently incorrect. The blog post also includes <a href="https://gist.github.com/karpathy/d4dee566867f8291f086">a minimal RNN code example</a>.</p>

<hr />

<blockquote>
  <details>
<summary>RNN Regularization, 2014 <a name="RNNRegularization"></a></summary> 
Paper <br /> 
Length: ~3,500 words<br /> 
Authors: Wojciech Zaremba, Ilya Sutskever and Oriol Vinyals <br />
<a href="#RNNRegularization">🔗</a>
</details>
</blockquote>

<p><a href="https://arxiv.org/abs/1409.2329">[RNN regularization, 2014]</a> addresses the challenge of training large RNNs without overfitting, where a model would excessively adapt to, or even memorize, its training samples and fail to generalize to new data. A technique for regularization, which aims to reduce this effect, was proposed that applies <em>dropout</em>, which omits randomly selected outputs of a given neural network layer. Dropout had been known for several years and was used e.g. in AlexNet. Here, the key insight was to utilize dropout only within a given RNN cell, but to avoid it on the recurrent connections that carried the hidden state vector. In this way, larger RNNs could avoid overfitting while preserving long-term dependencies.</p>

<hr />

<blockquote>
  <details>
<summary>Neural Turing Machines, 2014 <a name="NTM"></a></summary> 
Paper <br /> 
Length: ~7,500 words<br /> 
Authors: Alex Graves, Greg Wayne and Ivo Danihelka <br />
<a href="#NTM">🔗</a>
</details>
</blockquote>

<p><a href="https://arxiv.org/abs/1410.5401">[Neural Turing Machines, 2014]</a> were proposed as a form of memory-augmented neural network, with an external memory bank on which an RNN controller could write or erase information with a ‘blurry’, differentiable, attention-based focus. Equipped with this working memory, the Neural Turing Machine outperformed a baseline RNN in experiments involving associative recall, copying and sorting sequences and generalized more robustly to sequence lengths that exceeded those encountered in training.</p>

<hr />

<blockquote>
  <details>
<summary>Deep Speech 2, 2016 <a name="DeepSpeech"></a></summary> 
Paper <br /> 
Length: ~7,000 words<br /> 
Authors: Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, Jie Chen, Jingdong Chen, Zhijie Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Ke Ding, Niandong Du, Erich Elsen, Jesse Engel, Weiwei Fang, Linxi Fan, Christopher Fougner, Liang Gao, Caixia Gong, Awni Hannun, Tony Han, Lappi Johannes, Bing Jiang, Cai Ju, Billy Jun, Patrick LeGresley, Libby Lin, Junjie Liu, Yang Liu, Weigao Li, Xiangang Li, Dongpeng Ma, Sharan Narang, Andrew Ng, Sherjil Ozair, Yiping Peng, Ryan Prenger, Sheng Qian, Zongfeng Quan, Jonathan Raiman, Vinay Rao, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Kavya Srinet, Anuroop Sriram, Haiyuan Tang, Liliang Tang, Chong Wang, Jidong Wang, Kaifu Wang, Yi Wang, Zhijian Wang, Zhiqian Wang, Shuang Wu, Likai Wei, Bo Xiao, Wen Xie, Yan Xie, Dani Yogatama, Bin Yuan, Jun Zhan, Zhenyao Zhu  <br />
<a href="#DeepSpeech">🔗</a>
</details>
</blockquote>

<p><a href="http://proceedings.mlr.press/v48/amodei16.html?ref=https://codemonkey.link">[Deep Speech 2, 2016]</a> proposed an automatic speech recognition system to convert audio recordings into text by processing log-spectrograms representing the audio with RNNs to predict sequences of characters of either English or Mandarin. The authors utilized batch normalization instead of dropout for regularization on the non-recurrent layers and <em>Gated Recurrent Units</em> (GRUs) as a somewhat simplified alternative to LSTMs used in most of the other papers examined so far, together with a plethora of other engineering tweaks, including batched processing for low-latency streaming output.</p>

<hr />

<blockquote>
  <details>
<summary>RNNsearch, 2015 <a name="RNNsearch"></a></summary> 
Paper <br /> 
Length: ~8,000 words<br /> 
Authors: Dzmitry Bahdanau, KyungHyun Cho and Yoshua Bengio <br />
<a href="#RNNsearch">🔗</a>
</details>
</blockquote>

<p><a href="https://arxiv.org/abs/1409.0473">[RNNsearch]</a> is credited with introducing the first attention mechanism into <em>Natural Language Processing</em> (NLP), proposing additive, <em>content-based attention</em> for neural machine translation. Its encoder-decoder architecture encodes an input sequence of English words with an RNN encoder into a <em>context vector</em> used by an RNN <em>de</em>coder to predict an output sequence of French words. In prior work this context vector was simply the final hidden state of the encoder, which therefore had to contain all relevant information about the input sequence. RNNsearch addresses this bottleneck by making the context vector a weighted sum over <em>all</em> encoder hidden states, or <em>annotations</em>. When predicting a target word, the decoder can thereby rely on context from arbitrary parts of the encoded input sequence by (re-)calculating the context vector as a weighted sum of annotations. The weighting is determined by an <em>alignment model</em>, a feedforward network that receives the current decoder hidden state together with an annotation and assigns a score to the latter.</p>

<hr />

<blockquote>
  <details>
<summary>Pointer Networks, 2015 <a name="PtrNet"></a></summary> 
Paper <br /> 
Length: ~4,500 words<br /> 
Authors: Oriol Vinyals, Meire Fortunato and Navdeep Jaitly<br />
<a href="#PtrNet">🔗</a>
</details>
</blockquote>

<p><a href="https://proceedings.neurips.cc/paper_files/paper/2015/file/29921001f2f04bd3baee84a12e98098f-Paper.pdf">[Pointer Networks]</a> repurpose the concept of content-based attention to solve combinatorial optimization problems. Here, content-based attention is used to ‘point’ at elements of the input sequence in a specific order. The output sequence is therefore an indexing of the input elements. Given a set of two-dimensional points as input, Pointer Net was trained to solve for their <a href="https://en.wikipedia.org/wiki/Convex_hull">convex hull</a>, <a href="https://en.wikipedia.org/wiki/Delaunay_triangulation">Delaunay triangulation</a> or <a href="https://en.wikipedia.org/wiki/Travelling_salesman_problem">Traveling Salesman Problem</a> by predicting in which order these points should be visited. With no limitation to the length of the output sequence or dictionary, this approach was found to generalize beyond the longest sequence length encountered in training.</p>

<hr />

<blockquote>
  <details>
<summary>Set2Set, 2016 <a name="Set2Set"></a></summary> 
Paper <br /> 
Length: ~6,500 words<br /> 
Authors: Oriol Vinyals, Samy Bengio and Manjunath Kudlur<br />
<a href="#Set2Set">🔗</a>
</details>
</blockquote>

<p><a href="https://arxiv.org/abs/1511.06391">[Set2Set]</a> extends sequence-to-sequences methods as examined above to enable order-invariant processing of sets. These methods are shown to strongly depend on the specific order of both an input and output sequence (e.g. the exact order of random points provided to Pointer Networks for convex hull prediction). The authors propose Set2Set as a solution, with the encoder forming a <em>memory bank</em> (that resembles <em>annotations</em> of RNNSearch) to create a context vector. This memory bank, however, is sampled more than just once. Instead, a <em>process block</em> introduces a new LSTM, which evolves a <em>query vector</em> for repeated, content-based attention readouts of the memory. Finally, the <em>write block</em> (a Pointer Network) can add even more attention steps in the form of <em>glimpses</em>.</p>

<hr />

<blockquote>
  <details>
<summary>Relation Networks, 2017 <a name="RelationNetworks"></a></summary> 
Paper <br /> 
Length: 5,000 words<br /> 
Authors: Adam Santoro, David Raposo, David G. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap <br />
<a href="#RelationNetworks">🔗</a>
</details>
</blockquote>

<p><a href="https://proceedings.neurips.cc/paper_files/paper/2017/hash/e6acf4b0f69f6f6e60e9a815938aa1ff-Abstract.html">[Relation Network, 2017]</a> modules were proposed as a method for relational inference tasks such as visual and text-based question answering. The <em>Relation Network</em> module ingests a pair of feature vectors, for example an LSTM hidden state for a word or sentence in text or the values at a specific pixel position in feature maps produced by a CNN for image data. A given pairing is processed with one or more neural network layers before forming an element-wise sum and creating an output with a second stack of layers. By doing this for all pairs of inputs, this approach outperformed the human baseline in answering textual questions regarding the size, position and color of 3D generated shapes relative to each other.</p>

<hr />

<blockquote>
  <details>
<summary>Relational Recurrent Neural Networks, 2018 <a name="RelationalRNNs"></a></summary> 
Paper <br /> 
Length: 6,000 words<br /> 
Authors: Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap <br />
<a href="#RelationalRNNs">🔗</a>
</details>
</blockquote>

<p><a href="https://proceedings.neurips.cc/paper/2018/hash/e2eabaf96372e20a9e3d4b5f83723a61-Abstract.html">[Relational Recurrent Neural Networks, 2018]</a> proposed a <em>Relational Memory Core</em> module in which an attention mechanism allows memories to interact with each other and be recurrently refined as a fixed-size matrix. This approach was adapted for several tasks requiring relational reasoning and outperformed multiple baseline methods. Given a random set of vectors of which an arbitrary one was marked, it predicted which other vector had the highest Euclidean distance to it. It also learned to execute short code snippets involving variable manipulation, performed language modeling and scored well in a toy reinforcement learning task. The <em>self-attention</em> mechanism that enabled its memory interactions is described in the following section.</p>

<hr />

<h2 id="transformers">Transformers</h2>

<p>The previous section tracks the rise of attention mechanisms as an increasingly potent tool for providing context in sequence-to-sequence prediction tasks. Eventually, these developments yielded the Transformer as a neural network architecture that predominantly relies on attention and discards both recurrent and convolutional layers entirely. The excellent scalability of this approach, together with growing compute resources and extensive training data, established Transformers as dominant method for language modeling, forming the backbone of systems like ChatGPT and performing well even on image and multimodal data.</p>

<p>New attention mechanisms enabled <a href="https://ai.stackexchange.com/a/31584">substantial speed and efficiency advantages</a>:<br />
The <em>additive attention</em> mechanism of the previous section compared an encoder and decoder hidden state to each other by applying an alignment model to each such pair for scoring. Internally, the alignment model formed linear projections of both vectors and added them together to calculate a score, which was normalized over all encoder hidden states to form a context vector as their weighted average.<br />
With <em>multiplicative attention</em>, Transformers compare multiple pairings of hidden states at once by forming a dot product of their linear projections, which can be implemented with faster, highly optimized matrix multiplications.</p>

<hr />

<blockquote>
  <details>
<summary>Attention Is All You Need, 2017 <a name="AttentionIsAllYouNeed"></a></summary> 
Paper <br /> 
Length: ~4,500 words<br /> 
Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser and Illia Polosukhin<br />
<a href="#AttentionIsAllYouNeed">🔗</a>
</details>
</blockquote>

<p><a href="https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf">[Attention Is All You Need]</a> proposed the Transformer architecture. In an encoder-decoder structure for machine translation, embedding layers convert each input token into a feature vector, to which positional encodings are added. The proposed <em>Scaled Dot-Product Attention</em> computes a weighted average over multiple <em>value</em> vectors, each weighted by comparing its associated <em>key</em> vector to a given <em>query</em> vector using the dot product. The result is scaled (for numerical stability) and then normalized over all keys with a softmax function. <em>Multi-head attention</em> conducts this process in parallel with different, learned projections of each input.<br />
 Three variants of this mechanism are used. In <em>self-attention</em> used by the encoder, the <em>query</em>, <em>key</em> and <em>value</em> are distinct linear projections of the same output vector from the previous layer. In <em>masked self-attention</em> the decoder furthermore masks out the weights for future tokens. Finally, in <em>encoder-decoder attention</em>, each decoder block obtains only the <em>query</em> from the preceding decoder layer, whereas <em>key</em> and <em>value</em> originate from the final encoder layer. The experiments exceeded state-of-the-art results, with two orders of magnitude lower compute resources than previous approaches.</p>

<hr />

<blockquote>
  <details>
<summary>The Annotated Transformer, 2020 <a name="AnnotatedTransformer"></a></summary> 
Blog Post (2022 version)<br /> 
Length: ~6,000 words<br /> 
Authors: Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak, and Stella Biderman (2020 original by Sasha Rush)<br />
<a href="#AnnotatedTransformer">🔗</a>
</details>
</blockquote>

<p><a href="https://nlp.seas.harvard.edu/annotated-transformer/">[The Annotated Transformer]</a> implements the Transformer as described in <em>‘Attention is All You Need’</em> line by line as a fully functional Jupyter Notebook using PyTorch, with all <a href="https://github.com/harvardnlp/annotated-transformer/">code available on GitHub</a>. Text segments of the original paper feature alongside the code, together with comments and visualizations that clarify various aspects of the architecture beyond the contents of the paper. The notebook also implements examples for data formatting, training and inference that show the Transformer applied in practice.</p>

<p><em>Note: <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer by Jay Alammar</a> is yet another in-depth guide.</em></p>

<hr />

<blockquote>
  <details>
<summary>Scaling Laws for Neural Language Models, 2020 <a name="ScalingLaws"></a></summary> 
Paper<br /> 
Length: ~9,000 words<br /> 
Authors: Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu and Dario Amodei<br />
<a href="#ScalingLaws">🔗</a>
</details>
</blockquote>

<p><a href="https://arxiv.org/abs/2001.08361">[Scaling Laws for Neural Language Models]</a> explores the predictive performance of Transformers for language modeling as a function of model size, data quantity and available compute resources. Extensive empirical results enable the authors to establish formulas that relate these factors to each other over seven orders of magnitude and enable several recommendations as to their optimal configuration. While each of these can form a bottleneck, model size (i.e. the number of trainable parameters) forms the single most impactful factor. Larger models reach higher sample-efficiency and better generalization earlier on in training. The specific model architecture has little effect. An eight-fold increase in model size requires a five-fold increase in training data. Given a fixed compute budget, the authors accordingly recommend to prioritize first model size, then batch size and only then the number of training steps, with early stopping before convergence typically providing the best trade-off in their experiments.</p>

<hr />

<h2 id="information-theory">Information Theory</h2>

<p>A substantial portion of the reading list is dedicated to more abstract material on theoretical informatics. Rather than proposing specific architectures or engineering solutions for concrete applications, these works are concerned with more fundamental study of the limits of computability, probability and intelligence. Recurring themes are principles for inductive inference such as <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam’s razor</a>, which states a preference for simplicity when choosing between competing explanations (be they theories, hypotheses or models) for some given evidence or data. Another core concept is <a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity">Kolmogorov complexity</a>* for quantifying the amount of information, or potential for compression, of a given input.</p>

<p><em>*Note: Kolmogorov complexity of a sequence can be defined as the length of the shortest program that prints it and then halts. While uncomputable in practice, it can be approximated with compression software such as gzip.</em></p>

<hr />

<blockquote>
  <details>
<summary>A Tutorial Introduction to the Minimum Description Length Principle, 2004 <a name="MDL"></a></summary> 
Book Chapter <br /> 
Length: ~30,000 words<br /> 
Author: Peter Grünwald<br />
<a href="#MDL">🔗</a>
</details>
</blockquote>

<p><a href="https://arxiv.org/abs/math/0406077">[A Tutorial Introduction to the Minimum Description Length Principle, 2004]</a> 
describes an approach for model selection that mathematically formalizes Occam’s razor, defining a preference for the most simple model among all those that explain the available data. The principle relates learning to data compression, as the ability to exploit regularity for achieving a shortest possible description. This description is defined by codes, and codelength functions are noted as corresponding to probability mass functions. The <em>two-part code version</em> of the Minimum Description Length (MDL) principle measures the simplicity of a model instance as the length of its description (in bits) added to the length of the data description as encoded with it. The <em>refined, one-part code version</em> examines entire families of models based on their goodness-of-fit and complexity.</p>

<hr />

<blockquote>
  <details>
<summary>Kolmogorov Complexity and Algorithmic Randomness <a name="KomogorovComplexity"></a> <br /> (Chapter 14), 2017</summary> 
Book Chapter<br /> 
Length: ~35,000 words<br /> 
Authors: Alexander Shen, Vladimir A. Uspensky and Nikolay Vereshchagin<br />
<a href="#KolmogorovComplexity">🔗</a>
</details>
</blockquote>

<p><a href="https://www.lirmm.fr/~ashen/kolmbook-eng-scan.pdf">[Kolmogorov Complexity and Algorithmic Randomness]</a> features a final chapter on algorithmic statistics. In this framework, a given sequence of observations is encoded as one binary string. Kolmogorov complexity provides formal means of quantifying its randomness and regularity, as well as the expected and desired properties of a theory or model that can explain it. Such a model should preferably be simple, as indicated by low Kolmogorov complexity. It should also explain as much regularity in the data as possible, making the data “typical” for the model. This property is formally quantified by low <em>randomness deficiency</em> of the data relative to the model. Together, these two properties are also related to the two-part code of the Minimum Description Length principle. The chapter closes by drawing parallels between good models and good compressors, together with the potential of lossy compression to perform effective denoising.</p>

<hr />

<blockquote>
  <details>
<summary>The First Law of Complexodynamics, 2011 <a name="Complexodynamics"></a></summary> 
Blog Post <br /> 
Length: ~2,000 words<br /> 
Author: Scott Aaronson<br />
<a href="#Complexodynamics">🔗</a>
</details>
</blockquote>

<p><a href="https://scottaaronson.blog/?p=762">[The First Law of Complexodynamics]</a> explores the relationship between entropy and complexity. Whereas the second law of thermodynamics dictates that entropy of closed systems increases over time, their complexity of ‘interestingness’ is noted to first rise and then fall again. Giving the example of coffee and milk mixing in a glass, the highest such ‘complextropy’ is noted to occur midway, when tendrils of milk result from both liquids no longer being cleanly separated but also not yet forming a homogenous blend. Kolmogorov complexity is explored as a way to express both entropy and this ‘complextropy’, with the conjecture that a resource-bounded definition could provide a suitable theoretical framework.</p>

<hr />

<blockquote>
  <details>
<summary>Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton, 2014 <a name="RiseAndFallOfComplexity"></a></summary> 
Paper<br /> 
Length: ~8,500 words<br /> 
Authors: Scott Aaronson, Sean M. Carroll and Lauren Ouellette<br />
<a href="#RiseAndFallOfComplexity">🔗</a>
</details>
</blockquote>

<p><a href="https://arxiv.org/abs/1405.6903">[Quantifying the Rise and Fall of Complexity]</a> explores these ideas in further depth. Covering various theoretical notions of complexity, it eventually settles on ‘apparent complexity’ as a way of modeling the separate phenomena of entropy and the ‘interestingness’ of a closed system. Practical experiments inspired by the blending of coffee and milk fill a 2D array with a clean split of binary values and perturb these over multiple time steps to represent random mixing. This array forms an image which is compressed by gzip to approximate Kolmogorov complexity via file size. At each time step, this is done with the image itself to approximate entropy, but also with a coarse-grained, blurred version to estimate its apparent complexity. As envisioned, the increasingly noisy image values yield rising entropy whereas their blurred representation first raises and then decreases the apparent complexity measure as the mix gets more homogeneous.</p>

<hr />

<blockquote>
  <details>
<summary>Machine Super Intelligence, 2008 <a name="MachineSuperIntelligence"></a></summary> 
Dissertation <br /> 
Length: ~50,000 words<br /> 
Author: Shane Legg, supervised by Marcus Hutter<br />
<a href="#MachineSuperIntelligence">🔗</a>
</details>
</blockquote>

<p><a href="https://sonar.ch/usi/documents/317954">[Machine Super Intelligence, 2008]</a> explores universal artificial intelligence under aspects of algorithmic complexity, probability and information theory. It covers inductive inference from Epicurus principle of multiple explanations, Occam’s razor, Bayes rule and priors to complexity measures and agent-environment models as examined in reinforcement learning. Discussing various definitions and established tests for intelligence, it proposes a formal definition and measure for universal intelligence* as the ability of an agent to achieve specific goals in a wide range of environments. While the proposed measure itself is uncomputable in practice, it enables theoretical conclusions, such as the requirement that powerful agents be proportionally complex, and motivates several practical experiments in which a downscaled version of a hypothetically optimal agent is deployed for reinforcement learning.</p>

<p>*<em>Note: This measure would accordingly score the universal intelligence of specialized, ‘narrow’ machine learning systems that form the bulk of the papers examined in this blog post as comparatively low.</em></p>

<hr />

<h2 id="miscellaneous">Miscellaneous</h2>

<hr />

<blockquote>
  <details>
<summary>Keeping Neural Networks Simple by Minimizing the Description Length of the Weights, 1993 <a name="NNMDL"></a></summary> 
Paper <br /> 
Length: ~6,000 words<br /> 
Authors: Geoffrey E. Hinton and Drew van Camp<br />
<a href="#NNMDL">🔗</a>
</details>
</blockquote>

<p><a href="https://www.cs.toronto.edu/~hinton/absps/colt93.pdf">[Keeping Neural Networks Simple by Minimizing the Description Length of the Weights]</a> introduced the concept of Variational Inference with neural networks.  This approach enables neural network training to approximate the otherwise computationally prohibitive concept of Bayesian inference. The authors propose a regularization technique that represents each weight of a neural network as a Gaussian probability distribution described by a mean and a variance value. Inspired by the Minimum Description Length principle, the cost function used during training penalizes the description length of the weights and the data misfits. The authors argue that this representation allows for a substantial reduction in the description length of the weights. Their <em>Bits-Back Coding</em> argument states that the distribution of each weight can be sampled with random bits at no additional cost, as the random bits can be reconstructed given a fixed learning algorithm, architecture and initial probability distribution for each weight.</p>

<hr />

<blockquote>
  <details>
<summary>Variational Lossy Autencoder, 2017 <a name="LossyVAE"></a></summary> 
Paper <br /> 
Length: ~6,000 words<br /> 
Authors: Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever and Pieter Abbeel<br />
<a href="#LossyVAE">🔗</a>
</details>
</blockquote>

<p><a href="https://arxiv.org/abs/1611.02731">[Variational Lossy Autoencoders]</a> provide a way for data compression with control over which aspects of the data should be retained or discarded. In experimental results, this enables 2D image compression that discards local texture while retaining global structure. Autencoders use an inference model to compresses input data to a compact <em>latent code</em>, from which a generative model decodes the original input. This latent code should accordingly represent all information relevant for describing the input. When using sufficiently powerful autoregressive models like RNNs however, decoders had been previously found capable of predicting the output while ignoring the latent code entirely. Here, a theoretical explanation for this phenomenon is provided based on Bits Back Coding. The proposed approach weakens the decoder (e.g. limiting it to reconstruct small receptive fields) such that it depends on the missing information (e.g. global structure) being fully provided by the latent code to which the input is compressed.</p>

<hr />

<blockquote>
  <details>
<summary>GPipe, 2018 <a name="GPipe"></a></summary> 
Paper <br /> 
Length: ~5,000 words<br /> 
Authors: Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu and Zhifeng Chen<br />
<a href="#GPipe">🔗</a>
</details>
</blockquote>

<p><a href="https://arxiv.org/abs/1811.06965">[GPipe]</a> is* a library for distributed training of neural networks on more than one accelerator (e.g. GPUs). It subdivides the neural network architecture into <em>cells</em> formed by one or more consecutive layers and assigns each cell to a separate accelerator. It furthermore employs pipeline parallelism by also splitting each mini-batch of training samples into several <em>micro</em>-batches that are pipelined through, so that multiple accelerators can work on different micro batches concurrently. The gradients for all micro-batches are aggregated for one synchonous update per mini-batch. Training thus remains consistent regardless of cell count or micro-batch size.</p>

<p>*<em>Note: As for the library itself, <a href="https://github.com/kakaobrain/torchgpipe">GitHub</a> shows that its most recent commit for the final version v0.0.7 occurred in September 2020.</em></p>

<hr />

<blockquote>
  <details>
<summary>Neural Message Passing for Quantum Chemistry, 2017 <a name="NeuralMessagePassing"></a></summary> 
Paper <br /> 
Length: ~6,000 words<br /> 
Authors: Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals and George E. Dahl<br />
<a href="#NeuralMessagePassing">🔗</a>
</details>
</blockquote>

<p><a href="https://arxiv.org/abs/1704.01212">[Neural Message Passing for Quantum Chemistry]</a> explores the application of graph neural networks to predict quantum mechanical properties of organic molecules. The commonalities between several related works that utilize neural networks for graph data are first discussed and abstracted into a new concept of <em>Message Passing Neural Networks</em>. This framework considers undirected graphs composed of edges and nodes, both of which can have features. Each forward pass performs one or more steps of a message passing phase in which the hidden state of each given node is updated based on messages that depend on the hidden states and connecting edges with all adjacent nodes. Next, a readout phase calculates one hidden state for the entire graph using a readout function that is invariant to the order of graph nodes. Using the <a href="https://arxiv.org/abs/1511.05493">Gated Graph Neural Network</a> architecture, the authors present experimental results on graph data of molecular structures that achieved state-of-the-art results at the time.</p>

<hr />

<h2 id="concluding-thoughts">Concluding Thoughts</h2>

<p>Whereas this summary was written by hand, the described technology is approaching a point where its language modeling capabilities are hard to distinguish from human writing already now in 2024. Eventually, Large Language Models may indeed become self-explanatory in a literal sense. Prompting e.g. ChatGPT to summarize this material nonetheless still yields explanations that <em>seem</em> very convincing but can also be largely hallucinated and often misleading. Perhaps that will already improve once this article is ingested into the training data?</p>

<p>Although low-quality, generated content was a recurring theme I encountered while researching this list, there are also several independent summaries of the reading list worth sharing here:</p>
<ul>
  <li><a href="https://aman.ai/primers/ai/top-30-papers/">Aman Chadha’s <em>Distilled AI</em></a> (~12,000 words, includes other papers)</li>
  <li><a href="https://www.youtube.com/watch?v=oF6vcYHs6rw&amp;list=PL8hTotro6aVGtPgLJ_TMKe8C8MDhHBZ4W">DataMListic’s youtube video playlist</a> (about 25 min total)</li>
</ul>

<p>With this blog post, the known contents of the reading list are compressed to barely more than one percent of the original word count. This leaves a lot more to be discussed, but hopefully it still has something to offer for the interested reader. My own, subjective review will be saved for another post in the future.</p>

  </div>
  
</div>

<div class="pagination">
  
    <span class="pagination-item older">Older</span>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
